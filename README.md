# WattWise-FL: Federated, Privacy-Preserving Energy Forecasts for Smart Buildings

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

## ğŸ¯ Project Overview

WattWise-FL builds a classical ML pipeline to forecast hourly whole-building energy use while preserving privacy. Using the open BDG2 dataset (2016â€“2017), we predict `meter_reading` for electricity, chilled water, hot water, and steam using Ridge, LightGBM, and Explainable Boosting Machine (EBM) models trained under per-site, centralized, and federated learning regimes.

### Key Features
- âš¡ **Classical ML Focus**: Ridge, LightGBM, EBM (no deep learning)
- ğŸ” **Privacy-Preserving**: Federated learning simulation (FedAvg)
- ğŸ“Š **Rigorous Evaluation**: Time-series CV with unseen-site testing
- ğŸ” **Explainable AI**: EBM shape plots and feature importance
- ğŸŒ± **Sustainability Impact**: Enable peak shaving and emissions reduction

## ğŸ“ Project Structure

ML PROJECT/
â”‚
â”œâ”€â”€ README.md                          â† This file
â”œâ”€â”€ requirements.txt                   â† Python dependencies
â”œâ”€â”€ .gitignore                         â† Git ignore rules
â”œâ”€â”€ config.yaml                        â† Project configuration
â”œâ”€â”€ presentation_notes.md              â† Guide for class presentation
â”‚
â”œâ”€â”€ data/                              â† Data directory (git-ignored)
â”‚   â”œâ”€â”€ raw/                           â† Original BDG2 data
â”‚   â”œâ”€â”€ processed/                     â† Cleaned & processed data
â”‚   â””â”€â”€ interim/                       â† Intermediate data
â”‚
â”œâ”€â”€ src/                               â† Source code
â”‚   â”œâ”€â”€ step1_data_cleaning.py         â† Step 1 script
â”‚   â”œâ”€â”€ step2_feature_engineering.py   â† Step 2 script
â”‚   â”œâ”€â”€ step3_train_models.py          â† Step 3 script
â”‚   â”œâ”€â”€ app.py                         â† Streamlit Dashboard
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ download.py                â† Download BDG2 data
â”‚   â”‚   â””â”€â”€ preprocessing.py           â† Data cleaning functions
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ calendar_features.py       â† Time-based features
â”‚   â”‚   â”œâ”€â”€ weather_features.py        â† Weather features
â”‚   â”‚   â”œâ”€â”€ building_features.py       â† Building metadata features
â”‚   â”‚   â””â”€â”€ lag_features.py            â† Lag & rolling features
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ lightgbm_model.py          â† LightGBM wrapper
â”‚   â”‚   â”œâ”€â”€ ebm_model.py               â† EBM wrapper
â”‚   â”‚   â””â”€â”€ federated.py               â† Federated learning
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py                  â† Config loader
â”‚       â”œâ”€â”€ logging_utils.py           â† Logging setup
â”‚       â””â”€â”€ io.py                      â† File I/O utilities
â”‚
â””â”€â”€ models/                            â† Saved models (git-ignored)
```

## ğŸš€ Quick Start (The "Clean" Workflow)

We have organized the project into **3 clear steps** for reproducibility and clarity.

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Pipeline (3 Steps)

**Step 1: Data Cleaning**
Handles raw data ingestion, fixes wide-format issues, and merges metadata.
```bash
python src/step1_data_cleaning.py
```

**Step 2: Feature Engineering**
Generates calendar cycles, weather lags, and building interactions.
```bash
python src/step2_feature_engineering.py
```

**Step 3: Model Training**
Trains LightGBM, EBM, and runs the Federated Learning simulation.
```bash
python src/step3_train_models.py
```

### 3. Launch Dashboard
Visualize the results and explore the data interactively.
```bash
streamlit run src/app.py
```

## ğŸ† Results

Our models achieved state-of-the-art performance on the test set:

| Model | Accuracy (R2 Score) | Error (RMSLE) | Description |
|-------|-------------------|---------------|-------------|
| **LightGBM** | **97.9%** | **0.2695** | High-precision Gradient Boosting |
| **EBM** | **92.9%** | **0.4173** | Fully Explainable Additive Model |

*Note: Federated Learning simulation also completed successfully across 18 clients.*

## ğŸ“Š Dataset Information

**Building Data Genome 2 (BDG2)**
- **Source**: [BDG2 GitHub Repository](https://github.com/buds-lab/building-data-genome-project-2)
- **Buildings**: 1,636 non-residential buildings
- **Meters**: 3,053 energy meters
- **Time Period**: 2016-2017 (2 full years)
- **Frequency**: Hourly readings
- **Locations**: 19 sites across North America & Europe

**Citation**:
```
Miller, C., Kathirgamanathan, A., Picchetti, B. et al. 
The Building Data Genome Project 2, energy meter data from the ASHRAE 
Great Energy Predictor III competition. 
Sci Data 7, 368 (2020). 
https://doi.org/10.1038/s41597-020-00712-x
```

## ğŸ¯ Project Goals

1. **Forecasting**: Predict hourly `meter_reading` (kWh) for building energy consumption
2. **Privacy**: Implement federated learning to train models without sharing raw data
3. **Explainability**: Use EBM and interpretable methods for operator trust
4. **Validation**: Rigorous time-series CV + unseen-site testing
5. **Impact**: Enable peak shaving, load shifting, and emissions reduction

## ğŸ§ª Models & Approaches

### Baseline Models
- Naive: Same hour last week
- Rolling mean: 24-hour average
- Linear Regression
- Ridge Regression (with regularization)

### Main Models
- **LightGBM**: Gradient boosting decision trees
- **EBM**: Explainable Boosting Machine (interpretable additive model)

### Training Regimes
1. **Per-Site**: Train separate model for each building
2. **Centralized**: Pool all data, train one global model
3. **Federated**: FedAvg with local training, aggregate model updates

## ğŸ“ Evaluation Metrics

- **Primary**: RMSLE (Root Mean Squared Logarithmic Error)
- **Secondary**: MAE (Mean Absolute Error)
- **Diagnostics**: Error by season, temperature band, meter type, building type


## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

The BDG2 dataset is available under the CC BY 4.0 license.

## ğŸ™ Acknowledgments

- ASHRAE for organizing GEPIII competition
- BDG2 team for creating and maintaining the dataset
- Building owners who contributed their data for research

## ğŸ“§ Contact

For questions or collaboration opportunities, please contact the team members.


