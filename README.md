# WattWise-FL: Federated, Privacy-Preserving Energy Forecasts for Smart Buildings

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

## ğŸ¯ Project Overview

WattWise-FL builds a classical ML pipeline to forecast hourly whole-building energy use while preserving privacy. Using the open BDG2 dataset (2016â€“2017), we predict `meter_reading` for electricity, chilled water, hot water, and steam using Ridge, LightGBM, and Explainable Boosting Machine (EBM) models trained under per-site, centralized, and federated learning regimes.

### Key Features
- âš¡ **Classical ML Focus**: Ridge, LightGBM, EBM (no deep learning)
- ğŸ” **Privacy-Preserving**: Federated learning simulation (FedAvg)
- ğŸ“Š **Rigorous Evaluation**: Time-series CV with unseen-site testing
- ğŸ” **Explainable AI**: EBM shape plots and feature importance
- ğŸŒ± **Sustainability Impact**: Enable peak shaving and emissions reduction

## ğŸ“ Project Structure

```
ML PROJECT/
â”‚
â”œâ”€â”€ README.md                          â† This file
â”œâ”€â”€ requirements.txt                   â† Python dependencies
â”œâ”€â”€ environment.yml                    â† Conda environment (alternative)
â”œâ”€â”€ .gitignore                         â† Git ignore rules
â”œâ”€â”€ config.yaml                        â† Project configuration
â”‚
â”œâ”€â”€ data/                              â† Data directory (git-ignored)
â”‚   â”œâ”€â”€ raw/                           â† Original BDG2 data
â”‚   â”œâ”€â”€ processed/                     â† Cleaned & processed data
â”‚   â”œâ”€â”€ features/                      â† Feature matrices
â”‚   â””â”€â”€ splits/                        â† Train/test split indices
â”‚
â”œâ”€â”€ notebooks/                         â† Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_download.ipynb         â† Download BDG2 dataset
â”‚   â”œâ”€â”€ 02_eda_metadata.ipynb          â† Explore building metadata
â”‚   â”œâ”€â”€ 03_eda_weather.ipynb           â† Explore weather data
â”‚   â”œâ”€â”€ 04_eda_meters.ipynb            â† Explore meter readings
â”‚   â”œâ”€â”€ 05_preprocessing.ipynb         â† Data cleaning & QA
â”‚   â”œâ”€â”€ 06_feature_engineering.ipynb   â† Create features
â”‚   â”œâ”€â”€ 07_baseline_models.ipynb       â† Naive baselines & Ridge
â”‚   â”œâ”€â”€ 08_lightgbm_models.ipynb       â† LightGBM training
â”‚   â”œâ”€â”€ 09_ebm_models.ipynb            â† EBM training
â”‚   â”œâ”€â”€ 10_federated_learning.ipynb    â† FL simulation
â”‚   â””â”€â”€ 11_results_analysis.ipynb      â† Final results & plots
â”‚
â”œâ”€â”€ src/                               â† Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ download.py                â† Download BDG2 data
â”‚   â”‚   â”œâ”€â”€ preprocessing.py           â† Data cleaning functions
â”‚   â”‚   â””â”€â”€ validation.py              â† Data quality checks
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ calendar_features.py       â† Time-based features
â”‚   â”‚   â”œâ”€â”€ weather_features.py        â† Weather features
â”‚   â”‚   â”œâ”€â”€ building_features.py       â† Building metadata features
â”‚   â”‚   â””â”€â”€ lag_features.py            â† Lag & rolling features
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ baseline.py                â† Naive baselines
â”‚   â”‚   â”œâ”€â”€ ridge.py                   â† Ridge regression
â”‚   â”‚   â”œâ”€â”€ lightgbm_model.py          â† LightGBM wrapper
â”‚   â”‚   â”œâ”€â”€ ebm_model.py               â† EBM wrapper
â”‚   â”‚   â””â”€â”€ federated.py               â† Federated learning
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py                 â† RMSLE, MAE, etc.
â”‚   â”‚   â”œâ”€â”€ cv_splitter.py             â† Time-series CV
â”‚   â”‚   â””â”€â”€ diagnostics.py             â† Error analysis
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ plots.py                   â† Standard plots
â”‚   â”‚   â””â”€â”€ explainability.py          â† EBM, SHAP, PDP plots
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py                  â† Config loader
â”‚       â”œâ”€â”€ logging_utils.py           â† Logging setup
â”‚       â””â”€â”€ io.py                      â† File I/O utilities
â”‚
â”œâ”€â”€ models/                            â† Saved models (git-ignored)
â”‚   â”œâ”€â”€ baseline/
â”‚   â”œâ”€â”€ centralized/
â”‚   â”œâ”€â”€ per_site/
â”‚   â””â”€â”€ federated/
â”‚
â”œâ”€â”€ results/                           â† Results & figures
â”‚   â”œâ”€â”€ metrics/                       â† Metrics tables
â”‚   â”œâ”€â”€ figures/                       â† Plots & visualizations
â”‚   â””â”€â”€ model_cards/                   â† Model documentation
â”‚
â”œâ”€â”€ tests/                             â† Unit tests
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_features.py
â”‚   â””â”€â”€ test_models.py
â”‚
â””â”€â”€ docs/                              â† Documentation
    â”œâ”€â”€ data_dictionary.md
    â”œâ”€â”€ processing.md
    â””â”€â”€ model_cards_template.md
```

## ğŸš€ Quick Start (The "Clean" Workflow)

We have organized the project into **3 clear steps** for reproducibility and clarity.

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Pipeline (3 Steps)

**Step 1: Data Cleaning**
Handles raw data ingestion, fixes wide-format issues, and merges metadata.
```bash
python src/step1_data_cleaning.py
```

**Step 2: Feature Engineering**
Generates calendar cycles, weather lags, and building interactions.
```bash
python src/step2_feature_engineering.py
```

**Step 3: Model Training**
Trains LightGBM, EBM, and runs the Federated Learning simulation.
```bash
python src/step3_train_models.py
```

### 3. Launch Dashboard
Visualize the results and explore the data interactively.
```bash
streamlit run src/app.py
```

## ğŸ† Results

Our models achieved state-of-the-art performance on the test set:

| Model | Accuracy (R2 Score) | Error (RMSLE) | Description |
|-------|-------------------|---------------|-------------|
| **LightGBM** | **97.9%** | **0.2695** | High-precision Gradient Boosting |
| **EBM** | **92.9%** | **0.4173** | Fully Explainable Additive Model |

*Note: Federated Learning simulation also completed successfully across 18 clients.*

## ğŸ“Š Dataset Information

**Building Data Genome 2 (BDG2)**
- **Source**: [BDG2 GitHub Repository](https://github.com/buds-lab/building-data-genome-project-2)
- **Buildings**: 1,636 non-residential buildings
- **Meters**: 3,053 energy meters
- **Time Period**: 2016-2017 (2 full years)
- **Frequency**: Hourly readings
- **Locations**: 19 sites across North America & Europe

**Citation**:
```
Miller, C., Kathirgamanathan, A., Picchetti, B. et al. 
The Building Data Genome Project 2, energy meter data from the ASHRAE 
Great Energy Predictor III competition. 
Sci Data 7, 368 (2020). 
https://doi.org/10.1038/s41597-020-00712-x
```

## ğŸ¯ Project Goals

1. **Forecasting**: Predict hourly `meter_reading` (kWh) for building energy consumption
2. **Privacy**: Implement federated learning to train models without sharing raw data
3. **Explainability**: Use EBM and interpretable methods for operator trust
4. **Validation**: Rigorous time-series CV + unseen-site testing
5. **Impact**: Enable peak shaving, load shifting, and emissions reduction

## ğŸ§ª Models & Approaches

### Baseline Models
- Naive: Same hour last week
- Rolling mean: 24-hour average
- Linear Regression
- Ridge Regression (with regularization)

### Main Models
- **LightGBM**: Gradient boosting decision trees
- **EBM**: Explainable Boosting Machine (interpretable additive model)

### Training Regimes
1. **Per-Site**: Train separate model for each building
2. **Centralized**: Pool all data, train one global model
3. **Federated**: FedAvg with local training, aggregate model updates

## ğŸ“ Evaluation Metrics

- **Primary**: RMSLE (Root Mean Squared Logarithmic Error)
- **Secondary**: MAE (Mean Absolute Error)
- **Diagnostics**: Error by season, temperature band, meter type, building type


## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

The BDG2 dataset is available under the CC BY 4.0 license.

## ğŸ™ Acknowledgments

- ASHRAE for organizing GEPIII competition
- BDG2 team for creating and maintaining the dataset
- Building owners who contributed their data for research

## ğŸ“§ Contact

For questions or collaboration opportunities, please contact the team members.


